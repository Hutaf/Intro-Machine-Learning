{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fake News Detector\nBuilding a system to identify unreliable news articles."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make necessary imports\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\ndf = pd.read_csv('../input/fake-news/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rows and columns in the data\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a glimpse of the data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get more information about the data\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing data in each feature/column\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unrelated features first, then drop missing data\ndf = df.drop(columns=['title', 'author']).dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Double check missing data\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the labels. 1: unreliable, 0: reliable\nlabels = df.label\nlabels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(df['text'], labels, test_size = 0.2, random_state = 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.7)\n\n# Fit and transform train set, transform test set\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline(steps = [('clf', PassiveAggressiveClassifier())])\n                         \n                         \nsearch_space = [{'clf': [PassiveAggressiveClassifier()]},\n                {'clf': [MultinomialNB()]},\n                {'clf': [BernoulliNB()]},\n                {'clf': [RandomForestClassifier()]}]\n                         \ngridsearch = GridSearchCV(estimator=pipe,\n                          param_grid = search_space,\n                          scoring = 'accuracy')\n                         \nbest_model = gridsearch.fit(tfidf_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best accuracy: %f using %s'%(best_model.best_score_, best_model.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = best_model.predict(tfidf_test)\n\n# Build confusion matrix. 1: unreliable, 0: reliable\nconfusion_matrix(y_test, y_pred, labels=[1, 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\ntest_data = pd.read_csv('../input/fake-news/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign ids to an object to use it later for Kaggle submission\ntest_id = test_data['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rows and columns in the data\ntest_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a glimpse of the data\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get some information about the data\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many missing data in each feature/column\ntest_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unrelated features first, then fill missing data\n# Fill NAs instead of dropping, since the submission is expecting same number of observations as the original one\ntest_data = test_data.drop(columns=['id','title', 'author']).fillna('fake and unreliable')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Double check missing data\ntest_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confirm that cleaned test data has same observations as the original one (i.e. 5200)\ntest_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform test data\ntest_vectorized = tfidf_vectorizer.transform(test_data['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict test data\ntest_predictions = pac.predict(test_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join test data's ids with their respective predicted labels\nsubmission = pd.DataFrame({'id':test_id, 'label':test_predictions})\nsubmission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# Save the submission file\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}